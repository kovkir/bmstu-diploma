\chapter{Аналитический раздел}

% Иерархическая кластеризация категориальных данных в R
% https://habr.com/ru/company/otus/blog/461741/

% Иерархическая кластеризация с помощью Python и Scikit-Learn
% https://pythobyte.com/hierarchical-clustering-with-python-and-scikit-learn-768953fe/

% Готовим иерархическую кластеризацию или как я выявлял специализации у резюме
% https://habr.com/ru/company/hh/blog/427477/

% SciPy Hierarchical Clustering and Dendrogram Tutorial
% https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/


\section{Описание предметной области}

Кластеризация \cite{Clustering} --- это алгоритмический процесс разбиения набора данных на кластеры, состоящие из объектов, схожих между собой. При этом объекты из различных кластеров должны быть как можно более отличными друг от друга. Ключевой разницей кластеризации от классификации является отсутствие явно заданного набора групп, которое определяется в процессе выполнения алгоритма.

В кластеризации используются различные типы данных, которые могут быть разделены на две основные категории: количественные и категориальные. В зависимости от типа данных, могут быть использованы различные методы кластеризации.

Категориальные данные \cite{DataAnalysis} --- это набор данных, который хранит качественную информацию, закодированную в виде категорий. Категории не являются числами, а представляют собой описательные значения, такие как цвет, пол, марка автомобиля, рейтинг продукта и так далее.

Количественные данные \cite{DataAnalysis} --- это числовые данные, которые могут быть измерены и упорядочены. К ним относятся такие переменные, как возраст, рост, вес, расстояние, цена. Эти данные могут быть представлены как в дискретной, так и в непрерывной форме.

Сам процесс кластеризации состоит из трех шагов:

\begin{itemize}
    \item Построение матрицы несходства --- это важный этап в кластеризации, в котором определяется, насколько объекты отличаются друг от друга с помощью выбранной меры близости. 
    \item Выбор метода кластеризации --- определение подхода, который будет использоваться для группировки объектов на основе их сходства.
    \item Оценка и интерпретация --- оценка полученных результатов и их интерпретация в соответствии с поставленной задачей.
\end{itemize}

Каждый из этих шагов важен и может повлиять на результат кластеризации. Точность и эффективность разбиения зависят от правильного выбора метода кластеризации и меры близости при построении матрицы несходства.

\clearpage

\section{Методы разбиения}

% Обзор алгоритмов кластеризации данных (с таблицей сравнения!)
% https://habr.com/ru/post/101338/

% МЕТОДЫ КЛАСТЕРИЗАЦИИ МНОГОМЕРНЫХ СТАТИСТИЧЕСКИХ ДАННЫХ
% https://core.ac.uk/download/pdf/38644456.pdf

\subsection{Иерархический метод}

Иерархическая кластеризация \cite{ClassicalMethods} --- это метод разбиения данных, который создает вложенные кластеры путем их последовательного слияния или разделения. Каждый шаг приводит к увеличению кластеров или объединению уже существующих.

Процесс иерархической кластеризации можно визуализировать с помощью дендрограммы, которая представляет собой дерево с отдельными объектами на нижнем уровне и объединенными кластерами на более высоких уровнях. Каждый уровень дендрограммы представляет объединение объектов или кластеров на предыдущем уровне.

Этот метод построения кластеров подразделяется на два основных подхода:

\begin{itemize}
    \item агломеративный;
    \item дивизионный.
\end{itemize}

Преимущества иерархической кластеризации.

\begin{itemize}
    \item Для работы метода число кластеров не должно быть задано изначально.
    \item Дендрограмма, которая является результатом иерархической кластеризации, позволяет визуализировать объединение кластеров.
    \item Каждый шаг объединения кластеров сохраняется, что позволяет более детально изучать данные.
\end{itemize}

Недостатки иерархической кластеризации.

\begin{itemize}
    \item Различные меры близости могут давать различные результаты кластеризации, что может приводить к разным и нестабильным результатам.
    \item Для больших объемов данных возникают вычислительные проблемы, связанные с пересчетом расстояний между объектами на каждом этапе класстеризации.
\end{itemize}

\subsubsection{Агломеративный подход}

При агломеративном подходе \cite{HierarchicalСlustering} разбиение начинается с того, что каждый объект рассматривается как отдельный кластер. Затем на каждом шаге объединяются две наиболее близкие группы, пока все объекты не будут объединены в один кластер. 

Алгоритм агломеративного подхода иерархической кластеризации можно представить в следующем виде.

\begin{enumerate}
    \item Вычисление матрицы расстояний между всеми парами объектов (матрицы несходства). Для этого используется выбранная мера близости.
    \item Размещение каждого объекта в отдельный кластер.
    \item Выбор двух наиболее близких кластеров на основе матрицы несходства.
    \item Объединение выбранных кластеров в один новый кластер.
    \item Вычисление расстояния между новым кластером и всеми остальными кластерами (дополнение матрицы несходства).
    \item Повторение шагов 3-5 до тех пор, пока все объекты не будут объединены в один кластер.
    \item Построение дендрограммы, которая иллюстрирует процесс объединения кластеров.
\end{enumerate}

\subsubsection{Дивизионный подход}

При дивизионном подходе \cite{HierarchicalСlustering} разбиение начинается с того, что все объекты находятся в одном кластере. Затем на каждом шаге наиболее разнородная группа разделяется на два новых более гомогенных кластера. 

Алгоритм дивизионного подхода иерархической кластеризации можно представить в следующем виде.

\begin{enumerate}
    \item Вычисление матрицы расстояний между всеми парами объектов (матрицы несходства). Для этого используется выбранная мера близости.
    \item Размещение всех объектов в одном кластере.
    \item Выбор самого разнородного кластера на основе матрицы несходства.
    \item Разделение выбранного кластера на два новых более гомогенных кластера.
    \item Вычисление расстояния между новыми кластерами и всеми остальными кластерами (дополнение матрицы несходства).
    \item Повторение шагов 3-5 до тех пор, пока каждый объект не будет находиться в своем собственном кластере.
    \item Построение дендрограммы, которая иллюстрирует процесс разделения кластеров.
\end{enumerate}



\subsection{K-средних}

Метод k-средних \cite{KmeansСlustering} --- это один из наиболее распространенных методов кластеризации, который разбивает набор данных на заранее определенное число кластеров k. Каждый кластер в этом методе представлен своим центром.

Алгоритм можно представить в следующем виде.

\begin{enumerate}
    \item Выбирается число кластеров k.
    \item Случайным образом выбираются k центров кластеров.
    \item Для каждого объекта в наборе данных определяется ближайший к нему центр кластера (с помощью матрицы несходства).
    \item Для каждого кластера вычисляется центроид --- вектор, элементы которого представляют собой средние значения соответствующих признаков, вычисленных по всем записям кластера.
    \item Центроиды становятся новыми центрами кластеров.
    \item Шаги 3--5 повторяются до тех пор, пока центры кластеров не перестанут изменяться или алгоритм не превысит максимальное количество шагов.
    \item В результате каждому объекту будет присвоен номер кластера, к которому он наиболее близок.
\end{enumerate}

Преимущества метода разбиения k-средних.

\begin{itemize}
    \item Является одним из наиболее простых алгоритмов кластеризации.
    \item Не требует предварительной разметки данных.
    \item Можно получить результаты разбиения с использованием различных метрик качества кластеризации.
    \item Может масштабироваться для кластеризации больших объемов данных.
\end{itemize}

Недостатки метода разбиения k-средних.

\begin{itemize}
    \item Требуется заранее задать число кластеров k, что является недостатком в тех случаях, когда нет явной информации о количестве кластеров.
    \item Чувствителен к исходной инициализации центроидов кластеров, что может привести к различным результатам кластеризации при разных начальных условиях.
    \item Чувствителен к выбросам в данных. Если выбросы содержатся в кластере, то центр кластера будет смещаться ближе к ним и удаляться от большинства наблюдений в группе.
    \item Подходит только для вещественных чисел.
\end{itemize}



\subsection{K-режимов}

% The k-modes as Clustering Algorithm for Categorical Data Type
% https://medium.com/geekculture/the-k-modes-as-clustering-algorithm-for-categorical-data-type-bcde8f95efd7

Алгоритм k-режимов \cite{KmodesСlustering} представляет собой модификацию алгоритма k-средних.
Основная разница между двумя алгоритмами заключается в том, что k-средних разработан для работы с количественными числовыми данными, а k-режимов --- для работы с категориальными данными.

Алгоритм k-средних использует расстояние между объектами, чтобы определить близость между ними и сгруппировать их в кластеры. Расстояния обычно измеряются по евклидовой метрике.

Алгоритм k-режимов имеет схожий подход, но вместо расстояний использует меры сходства между объектами, которые основаны на анализе сходства между категориями. Чаще всего в качестве такой метрики используют расстояние Хэмминга.

Другое отличие между рассматриваемыми алгоритмами заключается в способе определения центров кластеров. Метод k-средних определяет центры путем вычисления среднего значения для каждого кластера, тогда как k-режимов определяет центр кластера как объект, который является наиболее типичным для кластера.

Алгоритм можно представить в следующем виде.

\begin{enumerate}
    \item Выбирается число кластеров k.
    \item Случайным образом выбираются k объектов из набора данных в качестве начальных центров кластеров.
    \item На каждой итерации каждый объект присваивается наиболее близкому по сходству к центру кластера (режиму). Измерение сходства в этом алгоритме осуществляется на основе расстояния Хэмминга.
    \item Для каждого режима рассчитывается наиболее часто встречающиеся значения каждого признака.
    \item Создаются новые режимы. Это делается путем выбора наиболее типичного объекта в данном кластере с учетом наиболее часто встречающихся значений каждого признака.
    \item Шаги 3--5 повторяются до тех пор, пока центры кластеров не перестанут изменяться или алгоритм не превысит максимальное количество шагов.
    \item В результате каждому объекту будет присвоен номер кластера, к которому он наиболее близок.
\end{enumerate}


Преимущества метода разбиения k-режимов.

\begin{itemize}
    \item Подходит для работы с категориальными данными.
    \item Простота реализации.
    \item Не требует метрики расстояния. Вместо этого он использует меру сходства между объектами на основе их категориальных признаков.
    \item Устойчив к выбросам, так как работает по принципу минимизации суммарного количества различий в категориях каждого кластера. Это означает, что он не зависит от расстояний между точками, а только от схожести категорий внутри кластеров. 
\end{itemize}

Недостатки метода разбиения k-режимов.

\begin{itemize}
    \item Требуется заранее задать число кластеров k, что является недостатком в тех случаях, когда нет явной информации о количестве кластеров.
    \item Чувствителен к исходной инициализации центроидов кластеров, что может привести к различным результатам кластеризации при разных начальных условиях.
    \item Проблемы в работе с разреженными данными.
    \item Не подходит для вещественных чисел.
\end{itemize}


\subsection{K-прототипов}

% https://www.hindawi.com/journals/mpe/2020/5143797/#sec4.4

Алгоритм кластеризации k-прототипов \cite{KprototypesСlustering} --- это комбинация метода k-средних и алгоритма k-режимов для кластеризации данных, содержащих как числовые, так и категориальные признаки. Он применяется для данных, где распределение признаков может быть как числовым (например, возраст, доход), так и категориальным (например, пол, цвет, марка автомобиля).

Алгоритм можно представить в следующем виде.

\begin{enumerate}
    \item Выбирается число кластеров k.
    \item Случайным образом выбираются k центров кластеров.
    \item Для каждого объекта в наборе данных определяется ближайший к нему центр кластера. Для вычисления матрицы несходства используются комбинированные метрики, которые учитывают расстояние между категориальными и числовыми данными.
    \item Для каждого кластера вычисляется центроид --- вектор, элементы которого представляют собой средние значения (для числовых переменных) или наиболее часто встречающиеся значения (для качественных переменных) соответствующих признаков, вычисленные по всем записям кластера.
    \item Центроиды становятся новыми центрами кластеров.
    \item Шаги 3--5 повторяются до тех пор, пока центры кластеров не перестанут изменяться или алгоритм не превысит максимальное количество шагов.
    \item В результате каждому объекту будет присвоен номер кластера, к которому он наиболее близок.
\end{enumerate}

Преимущества метода разбиения k-прототипов.

\begin{itemize}
    \item Может обрабатывать данные, содержащие как числовые, так и категориальные признаки.
    \item Простота реализации.
    \item Более устойчив к выбросам, чем алгориитм k-средних, в случае работы с категориальными данными, так как он не будет зависеть от расстояний между точками, а только от схожести категорий внутри кластеров (как и алгоритм k-режимов).
\end{itemize}

Недостатки метода разбиения k-прототипов.

\begin{itemize}
    \item Требуется заранее задать число кластеров k, что является недостатком в тех случаях, когда нет явной информации о количестве кластеров.
    \item Чувствителен к исходной инициализации центроидов кластеров, что может привести к различным результатам кластеризации при разных начальных условиях.
\end{itemize}


\subsection{C-средних}

Метод разбиения c-средних \cite{CmeansСlustering} представляет собой обобщение метода кластеризации k-средних в ситуации, когда точки данных могут принадлежать нескольким кластерам одновременно. Данный метод относится к нечетким алгоритмам кластеризации. При нечетком разбиении каждая точка имеет вероятность принадлежности к каждому кластеру, а не полностью принадлежит только одной группе.

Алгоритм можно представить в следующем виде.

\begin{enumerate}
    \item Выбирается число кластеров k.
    \item Случайным образом выбираются k центров кластеров.
    \item Рассчитывается расстояние между каждой точкой данных и каждым центром кластера (с помощью матрицы несходства).
    \item На основе вычисленных расстояний определяется степень (вероятность) принадлежности каждой точки конкретному кластеру (в диапазоне от 0 до 1).
    \item Определяются новые центры кластеров на основе вычисленных степеней принадлежности.
    \item Шаги 3--5 повторяются до достижения условия сходимости (например, пока сумма квадратов расстояний между объектами и центрами кластеров не перестанет изменяться).
    \item В результате, каждая точка данных получит степень принадлежности к каждому из кластеров.
\end{enumerate}

Преимущества метода разбиения c-средних.

\begin{itemize}
    \item Позволяет определять нечеткую принадлежность каждой точки данных к кластеру.
    \item Имеет возможность задавать коэффициент нечеткости, который определяет степень размытости кластеров. 
    \item Более устойчив к шумам в данных, чем жесткие алгоритмы разбиения, так как объекты имеют определенную степень принадлежности к каждому кластеру, а не закреплены за отдельной группой.
    \item Более устойчив к выбору начальных центров, чем алгоритм k-средних, благодаря нечеткой принадлежности точек к кластерам.
\end{itemize}

Недостатки метода разбиения c-средних.

\begin{itemize}
    \item Требуется заранее задать число кластеров k, что является недостатком в тех случаях, когда нет явной информации о количестве кластеров.
    \item Определение оптимального значения параметра нечеткости (m) требует дополнительного тестирования.
    \item Имеет большую вычислительную сложность, чем классический алгоритм k-средних.
\end{itemize}


\subsection{DBSCAN}

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) \cite{DBSCANСlustering} --- это метод кластеризации, который использует плотность данных для группировки объектов в кластеры. Основной идеей DBSCAN является нахождение областей высокой плотности точек в данных, и разделение их на отдельные кластеры. Данный метод позволяет находить кластеры произвольной формы в данных, не требуя заранее заданного количества групп. 

На вход DBSCAN помимо матрицы несходства принимает два основных параметра: радиус $\epsilon$-окрестности и минимальное количество соседних точек в пределах заданного радиуса (minPts). Эти параметры определяют границы и ширину области высокой плотности точек, которые должны быть объединены в кластеры.

Алгоритм можно представить в следующем виде.

\begin{enumerate}
    \item Выбирается случайная точка из нерассмотренных объектов данных.
    \item Вычисляется количество соседей вокруг выбранной точки на расстоянии, не превышающем заданный радиус $\epsilon$, и проверяется, достаточно ли этого количества соседей для добавления точки в кластер.
    \item Если количество соседей больше или равно заданному порогу minPts, точка считается <<основной>> и добавляется в текущий кластер. Также просматриваются все соседи этой точки, и если какой-то из них также является <<основной>> точкой, то он добавляется в текущий кластер.
    \item Если количество соседей меньше порога minPts, но точка находится в радиусе $\epsilon$ от другой <<основной>> точки, она считается <<граничной>> и добавляется в текущий кластер как граничная. Однако, эта точка не исследуется дальше на наличие <<основных>> соседей и не может стать их источником.
    \item Если точка не имеет достаточно соседей на установленном расстоянии $\epsilon$, и не находится в окружении граничных точек, она считается <<шумом>> и отбрасывается.
    \item Повторяется процесс до тех пор, пока все объекты в данных не будут просмотрены.
    \item Если два кластера имеют пересечение, то они объединяются в один.
    \item В итоге получается набор кластеров и точки, которые не вошли ни в один кластер, и считаются шумом.
\end{enumerate}

Преимущества метода разбиения DBSCAN.

\begin{itemize}
    \item Устойчивость к шуму и выбросам, так как данный метод позволяет обрабатывать и отделять точки, которые не принадлежат кластерам и являются выбросами.
    \item Нет необходимости знать количество кластеров до начала разбиения.
    \item Может обрабатывать кластеры произвольной формы и размеров.
    \item Может обрабатывать данные, содержащие как числовые, так и категориальные признаки.
\end{itemize}

Недостатки метода разбиения DBSCAN.

\begin{itemize}
    \item Чувствительность к выбору начальных параметров: радиусу $\epsilon$-окрестности и минимальному количеству соседних точек в пределах заданного радиуса (minPts).
    \item Не может обработать данные с различной плотностью.
\end{itemize}


\subsection{Минимальное покрывающее дерево}

Кластеризация на основе минимального покрывающего дерева \cite{AnalysisСlusteringAlgorithms} (МПД) --- это метод кластеризации, который основывается на построении минимального остовного дерева (МОД) \cite{MinimumSpanningTree} для набора данных. МОД представляет собой подмножество ребер связного графа, которое соединяет все вершины графа и имеет минимальную сумму весов. Кластеры образуются путем удаления ребер, длина которых превышает пороговое значение, из минимального остовного дерева.

Алгоритм можно представить в следующем виде.

\begin{enumerate}
    \item Строится полный граф данных, где каждый объект представлен вершиной, а вес ребра между двумя вершинами равен расстоянию между ними.
    \item Строится минимальное остовное дерево графа, используя алгоритм Прима или Крускала.
    \item Остовное дерево разделяется на кластеры путем удаления ребер с весами, превышающими пороговое значение.
    \item В результате каждая отрезанная ветвь дерева будет представлять собой отдельный кластер.
\end{enumerate}

Преимущества метода разбиения на основе минимального покрывающего дерева.

\begin{itemize}
    \item Нет необходимости знать количество кластеров до начала разбиения.
    \item Является устойчивым к шуму, так как использует МОД. В результате работы алгоритма ребра с большими весами будут отброшены.
    \item Каждая отрезанная ветвь дерева показывает иерархические отношения между объектами в кластере, что позволяет получить более четкое представление о свойствах данных.
    \item Может обрабатывать данные, содержащие как числовые, так и категориальные признаки.
\end{itemize}

Недостатки метода разбиения на основе минимального покрывающего дерева.

\begin{itemize}
    \item Необходимо заранее знать пороговое значение расстояний между объектами.
    \item Не обеспечивает равномерное разбиение на кластеры.
\end{itemize}


\subsection{Сравнение методов разбиения}

Сравнение методов разбиения данных предлагается проводить по следующим критериям.

\begin{enumerate}
    \item Возможность обрабатывать данные, содержащие числовые значения.
    \item Возможность обрабатывать данные, содержащие категориальные признаки.
    \item Нет необходимости знать количество кластеров до начала разбиения.
    \item Тип алгоритма кластеризации.
    \item Форма кластеров после применения алгоритма.
    \item Входные данные.
    \item Выходные данные.
\end{enumerate}

\clearpage
Результаты сравнения методов класстеризации данных приведены в таблицах \ref{tbl:compare_clustering1}--\ref{tbl:compare_clustering2}.

\begin{table}[H]
    \centering
	\caption{Сравнение рассмотренных методов разбиения данных (часть 1)}
    \label{tbl:compare_clustering1}
	\begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Крит.} & \textbf{Иерархический} & \textbf{K-средних} & \textbf{K-режимов} & \textbf{K-прототипов}
        \\ \hline

        Кр. 1 & + & + & -- & + \\ \hline
        Кр. 2 & + & -- & + & + \\ \hline
        Кр. 3 & + & -- & -- & -- \\ \hline
        Кр. 4 & Иерархический & Центроидный & Центроидный & Центроидный \\ \hline
        Кр. 5 & Произвольная & Гиперсфера & Гиперсфера & Гиперсфера \\ \hline
        Кр. 6 & \begin{minipage}[t]{3.5cm}\centering Матрица несходства, число кластеров (необязательно)\end{minipage} 
              & \begin{minipage}[t]{3.1cm}\centering Массив объектов, число кластеров\end{minipage} 
              & \begin{minipage}[t]{3.1cm}\centering Массив объектов, число кластеров\end{minipage} 
              & \begin{minipage}[t]{3.1cm}\centering Массив объектов, число кластеров\end{minipage} 
            \\ \hline
        Кр. 7 & \begin{minipage}[t]{3.5cm}\centering Бинарное дерево кластеров\end{minipage} 
              & \begin{minipage}[t]{3.1cm}\centering Кластеры с номерами объектов\end{minipage}
              & \begin{minipage}[t]{3.1cm}\centering Кластеры с номерами объектов\end{minipage} 
              & \begin{minipage}[t]{3.1cm}\centering Кластеры с номерами объектов\end{minipage} 
            \\ \hline
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
	\caption{Сравнение рассмотренных методов разбиения данных (часть 2)}
    \label{tbl:compare_clustering2}
	\begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Крит.} & \textbf{C-средних} & \textbf{DBSCAN} & \textbf{МПД}
        \\ \hline

        Кр. 1 & + & + & + \\ \hline
        Кр. 2 & -- & + & + \\ \hline
        Кр. 3 & -- & + & + \\ \hline
        Кр. 4 & Центроидный & На основе плотности & На основе графов \\ \hline
        Кр. 5 & Гиперсфера & Неравномерная & Произвольная \\ \hline
        Кр. 6 & \begin{minipage}[t]{4.7cm}\centering Массив объектов, число кластеров\end{minipage} 
              & \begin{minipage}[t]{4.7cm}\centering Матрица несходства, радиус $\epsilon$, кол-во соседей minPts\end{minipage} 
              & \begin{minipage}[t]{4.6cm}\centering Матрица несходства, пороговое значение расстояний\end{minipage} 
            \\ \hline
        Кр. 7 & \begin{minipage}[t]{4.7cm}\centering Центры кластеров, матрица принадлежности объектов к кластерам\end{minipage} 
              & \begin{minipage}[t]{4.7cm}\centering Набор кластеров и точки, которые не вошли ни в один кластер\end{minipage} 
              & \begin{minipage}[t]{4.6cm}\centering Древовидная структура кластеров\end{minipage} 
            \\ \hline
    \end{tabular}
\end{table}


\section{Критерии связи}

В случае использования иерархических алгоритмов встает вопрос, как объединять между собой кластеры, как вычислять <<расстояния>> между ними. Для решения этой задачи используются различные критерии связи \cite{Links}, которые определяют правило, по которому происходит объединение групп. Выбор критерия связи зависит от природы данных, целей исследования и типа кластеров, которые нужно выделить. Рассмотрим самые популярные метрики.

% Кластерный анализ
% http://statsoft.ru/home/textbook/modules/stcluan.html

\textbf{Одиночная связь} \cite{SingleLink}. Расстояние между двумя кластерами --- кратчайшее расстояние между двумя точками в каждом кластере.

Такая связь считается простой в реализации, но может приводить к образованию длинных цепочек объектов. Это происходит потому, что связь не учитывает сходство между всеми объектами внутри каждого кластера и может объединять две группы, состоящие из объектов, не похожих друг на друга. Данный метод подвержен проблемам шума. 

\textbf{Полная связь} \cite{CompleteLink}. Расстояние между двумя кластерами --- самое длинное расстояние между двумя точками в каждом кластере.

Этот критерий связи обеспечивает более однородные и компактные кластеры, чем одиночная связь.
Полная связь учитывает расстояние между объектами внутри каждой группы, что делает его менее чувствительным к выбросам. Однако этот метод более затратен по вычислительным ресурсам (чем одиночная связь) из-за того, что требует вычисления расстояний между всеми парами объектов внутри каждого кластера. Также полная связь может привести к объединению групп с большим числом объектов.

\textbf{Средняя связь} \cite{AverageLink}. Расстояние между двумя кластерами --- это среднее расстояние между каждой точкой в одном кластере до каждой точки в другом кластере.

Данный метод уменьшает вероятность объединения кластеров с большим числом объектов. Связь по средним значениям учитывает не только расстояние между ближайшими объектами внутри каждого кластера, но и среднее расстояние между всеми парами объектов. Такой подход обеспечивает большую устойчивость к выбросам, чем односвязная связь. Однако объединение кластеров может быть не таким компактным, как при использовании полной связи.

% https://habr.com/ru/post/101338/
% (есть и другие)


\section{Меры расстояний}

В кластеризации используются различные меры расстояний для определения схожести между объектами (точками, векторами) и расчета расстояний между кластерами \cite{AnalysisСlusteringAlgorithms}.

Выбор меры расстояний в кластеризации в значительной степени зависит от типа данных и задачи, которую мы решаем. Некоторые меры расстояний могут применяться только с определенными типами данных. Например, евклидово расстояние работает на непрерывных числовых данных, но не подходит для категориальных и бинарных признаков. Число измерений в данных также может повлиять на выбор меры расстояний. Например, расстояние Манхэттен лучше себя показывает на большом количестве признаков. 

Различные меры расстояний могут приводить к различным результатам кластеризации. Рассмотрим самые популярные из них.

\subsection{Евклидово расстояние}

Евклидово расстояние \cite{EuclideanDistance} является одной из наиболее распространенных мер расстояний, используемых в кластеризации. Эта метрика позволяет измерять расстояние между двумя точками в многомерном пространстве. Для двух точек $X$ и $Y$ в $n$-мерном пространстве формула евклидова расстояния определяется следующим образом:
\begin{equation}
    d(X,Y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \cdots + (x_n - y_n)^2},
\end{equation}
где $x_1, x_2, \ldots, x_n$ и $y_1, y_2, \ldots, y_n$ --- координаты точек $X$ и $Y$ соответственно.

Евклидово расстояние широко используется для кластеризации данных в пространстве вещественных чисел. Однако оно не подходит для работы с категориальными и бинарными данными, из-за нарушения условия непрерывности признаков.

\subsection{Квадрат евклидова расстояния}

Квадрат евклидова расстояния \cite{EuclideanDistance} --- это мера расстояний между двумя точками, которая измеряется как сумма квадратов разницы между координатами точек на каждом измерении. Для двух точек $X$ и $Y$ в $n$-мерном пространстве формула квадрата евклидова расстояния определяется следующим образом:
\begin{equation}
    d^2(X,Y) = (x_1-y_1)^2 + (x_2-y_2)^2 + \cdots + (x_n-y_n)^2,
\end{equation}
где $x_1,x_2,\ldots,x_n$ и $y_1,y_2,\ldots,y_n$ --- координаты точек $X$ и $Y$ соответственно.

Квадрат евклидова расстояния также широко используется в кластеризации данных. Он менее чувствителен к выбросам, чем обычное евклидово расстояние. Выбор данной меры может привести к потере точности в случае, если значения признаков сильно различаются. Квадрат евклидова расстояния также подходит только для работы с числовыми данными.

\subsection{Расстояние городских кварталов}

Расстояние городских кварталов (Манхэттенское расстояние) \cite{ManhattanDistance} --- это мера расстояния между двумя точками, измеряемая как сумма разницы между координатами точек на каждом измерении. Для двух точек $X$ и $Y$ в $n$-мерном пространстве формула расстояния городских кварталов определяется следующим образом:
\begin{equation}
    d(X,Y) = |x_1-y_1| + |x_2-y_2| + \cdots + |x_n-y_n|,
\end{equation}
где $x_1,x_2,\ldots,x_n$ и $y_1,y_2,\ldots,y_n$ --- координаты точек $X$ и $Y$ соответственно. 

Такое расстояние названо в честь структурных особенностей городских кварталов: движение разрешено только вправо и вверх на квадратной сетке, ограниченной городскими кварталами, нельзя перемещаться по кратчайшему прямому пути.

Манхэттенское расстояние учитывает не только разность значений признаков, но и их взаимное расположение, что в некоторых случаях может быть более значимым. Это расстояние используется для кластеризации точек в пространстве и не подходит для работы с категориальными данными. Недостатком является то, что расстояние городских кварталов не может учитывать важность разных признаков и предполагает, что каждый признак имеет одинаковый вес.

\subsection{Расстояние Чебышева}

Расстояние Чебышева \cite{9DistanceMeasures} --- это мера расстояния между двумя точками, которая используется в кластеризации и определяется как максимальное значение разницы между значениями двух точек по каждому измерению. Для двух точек $X$ и $Y$ в $n$-мерном пространстве расстояние Чебышева можно определить следующим образом:
\begin{equation}
    d(X,Y) = \max_{i=1}^n |x_i-y_i|,
\end{equation}
где $x_i$ и $y_i$ --- координаты $i$-того измерения точек $X$ и $Y$ соответственно. 

Расстояние Чебышева выбирается для работы в тех случаях, когда важно учитывать максимальное отклонение каждой координаты каждого объекта в группе. Методы кластеризации, использующие данное расстояние, устойчивы к выбросам в данных. Измерение максимального отклонения справедливо только для числовых значений, поэтому расстояние Чебышева не подходит для работы с категориальными данными.

\subsection{Расстояние Минковского}

Расстояние Минковского \cite{MinkowskiDistance} --- это обобщение евклидова расстояния и расстояния городских кварталов. Для двух точек $X$ и $Y$ в $n$-мерном пространстве формула расстояния Минковского порядка $p$ определяется следующим образом:
\begin{equation}
    d(X,Y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/p},\ p \geq 1, 
\end{equation}
где $x_i$ и $y_i$ --- координаты $i$-того измерения точек $X$ и $Y$ соответственно, $p$ --- параметр степени, который определяет, как взаимодействуют различные признаки при вычислении расстояния.

Когда $p=1$, формула расстояния Минковского дает оценку расстояние Манхэттена, когда $p=2$, она дает евклидово расстояние, при $p=\infty$ метрика обращается в расстояние Чебышева.

\subsection{Степенное расстояние}

Степенное расстояние \cite{AnalysisСlusteringAlgorithms} --- это обобщение формулы расстояния Минковского. Для двух точек $X$ и $Y$ в $n$-мерном пространстве формула степенного расстояния определяется следующим образом:
\begin{equation}
    d(X,Y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/r},\ p \geq 1,\ r \geq 1, 
\end{equation}
где $x_i$ и $y_i$ --- координаты $i$-того измерения точек $X$ и $Y$ соответственно, $p$ --- показатель степени, определяющий, как сильно каждый признак учитывается в общем расстоянии, $r$ --- параметр, который определяет масштаб расстояния.

Степенное расстояние используется для кластеризации числовых данных. Данная метрика используется для измерения расстояния между объектами на основе нелинейных отношений между признаками. 

\subsection{Расстояние Хэмминга}

Расстояние Хэмминга \cite{HammingDistance} --- мера сходства между объектами, которая основана на анализе сходства между категориями. Данная метрика показывает количество различающихся позиций для строк с одинаковой длинной.
Формула расстояния Хэмминга в кластеризации для двух векторов $\vec{x}$ и $\vec{y}$ длины $n$ может быть записана следующим образом:
\begin{equation}
    d(\vec{x},\vec{y}) = \sum_{i=1}^{n} \text{sign}|x_i - y_i|, 
\end{equation}
где $x_i$ и $y_i$ --- значения $i$-ых элемента векторов $\vec{x}$ и $\vec{y}$ соответственно. 

Таким образом, расстояние Хэмминга подходит для работы с бинарными данными. Для категориальных данных оно может быть рассчитано на основе количества несовпадений категорий между объектами. Однако для работы с числовыми данными следует выбирать другое расстояние.

\subsection{Расстояние Говера}

Расстояние Говера \cite{GowerDistance} --- метрика сходства в кластеризации, позволяющая выполнять расчет расстояний между объектами, содержащими как числовые, так и категориальные признаки.
Формальная запись расстояния Говера для двух векторов $\vec{x_i} = (x_{i1},x_{i2},\ldots,x_{ip})$ и $\vec{x_j} = (x_{j1},x_{j2},\ldots,x_{jp})$ может быть записана следующим образом:
\begin{equation}
    d(\vec{x_i},\vec{x_j}) = \frac{\sum_{k=1}^{p} s_{ijk} \cdot \delta_{ijk}}{\sum_{k=1}^{p} \delta_{ijk}}, 
\end{equation}
где $\delta_{ijk}$ --- величина, принимающая значение 1, если $\vec{x}$ и $\vec{y}$ можно сравнить по $k$-ому признаку, и 0, если сравнить нельзя, $s_{ijk} \in [0,1]$ --- оценка сходства двух признаков (чем признаки ближе друг к другу, тем оценка ближе к 0), $p$ --- количество признаков в векторах.

Формула расстояния Говера, когда отсутствующих значений не существует, может быть записана следующим образом:
\begin{equation}
    d(\vec{x_i},\vec{x_j}) = \frac{\sum_{k=1}^{p} s_{ijk}}{p},
\end{equation}

Оценка сходства двух признаков $s_{ijk}$ определяется по-разному для каждого типа данных.

\begin{enumerate}
    \item Для числовых перемеенных:
    \begin{equation}
        s_{ijk} = \frac{|x_{ik} - x_{jk}|}{R_k},
    \end{equation}
    где $R_k$ --- диапазон значений признака $k$, $x_{ik}$, $x_{jk}$ --- значения $k$-ых признаков векторов $\vec{x_i}$, $\vec{x_j}$ соответственно.
    \item Для категориальных переменных:
    \begin{equation}
        s_{ijk} =
        \begin{cases}
            0, & \text{если $x_{ik} = x_{jk}$}; \\
            1, & \text{иначе},
        \end{cases}
    \end{equation}
    где $x_{ik}$, $x_{jk}$ --- значения $k$-ых признаков векторов $\vec{x_i}$, $\vec{x_j}$ соответственно.
\end{enumerate}

\subsection{Сравнение мер расстояний}

Сравнение мер расстоняний предлагается проводить по следующим критериям.

\begin{enumerate}
    \item Возможность находить расстояние между числовыми данными.
    \item Возможность находить расстояние между категориальными данными (без предварительной обработки категориальных признаков и кодирования качественных параметров числами).
\end{enumerate}

Результаты сравнения мер расстоняний приведены в таблице \ref{tbl:compare_distance}.
\begin{table}[H]
    \centering
	\caption{Сравнение рассмотренных мер расстоняний}
    \label{tbl:compare_distance}
	\begin{tabular}{|c|c|c|}
        \hline
        \textbf{Мера расстоняний} & \textbf{Критерий 1} & \textbf{Критерий 2}
        \\ \hline

        Евклидово расстояние & + & - \\ \hline
        Квадрат евклидова расстояния & + & - \\ \hline
        Расстояние городских кварталов & + & - \\ \hline
        Расстояние Чебышева & + & - \\ \hline
        Расстояние Минковского & + & - \\ \hline
        Степенное расстояние & + & - \\ \hline
        Расстояние Хэмминга & - & + \\ \hline
        Расстояние Говера & + & + \\ \hline
    
    \end{tabular}
\end{table}


\section{Методы оценки качества кластеризации}

% Кластеризуем лучше, чем <<метод локтя>>
% https://habr.com/ru/company/jetinfosystems/blog/467745/

\subsection{Метод локтя}

Метод локтя \cite{ElbowMethod} --- один из самых распространенных методов оценки качества разбиения, который используется для определения оптимального количества кластеров. Этот метод получил своё название по форме графика зависимости среднего расстоняния в пределах группы от количества кластеров.

Идея метода заключается в том, чтобы найти на графике такую точку (точку <<локтя>>), после которой уменьшение среднего расстоняния между элементами в кластере будет не так заметно. Оптимальное число кластеров будет находиться в точке <<локтя>>. 

Этот метод  оценки качества разбиения применяется, если важнейшим фактором для анализа является компактность кластеров, то есть сходство внутри групп.

\subsection{Метод оценки силуэтов}

% https://pcnews.ru/blogs/%5Bperevod%5D_ierarhiceskaa_klasterizacia_kategorialnyh_dannyh_v_r-914880.html#gsc.tab=0

Метод оценки силуэтов \cite{SilhouetteMethod} --- это метод оценки качества разбиения, основанный на вычислении коэффициента силуэта для каждого объекта набора данных. График силуэтов показывает, насколько близко каждая точка внутри одной группы расположена к точкам ближайшего соседнего кластера.

Коэффициент силуэта для каждого объекта $i$ может быть вычислен следующим образом:
\begin{equation}
    s_i = \frac{b_i - a_i}{max(a_i,b_i)}, 
\end{equation}
где $a_i$ --- среднее расстояние между объектом $i$ и другими объектами в том же кластере, $b_i$ --- среднее расстояние между объектом $i$ и объектами из ближайшего кластера.

Значения коэффициентов силуэтов $s_i$ находятся в диапазоне $[-1, 1]$ и могут быть интерпретированы следующим образом:
\begin{itemize}
    \item если значение близко к 1, то объект находится в хорошо разделенном кластере;
    \item если значение близко к -1, то объект ошибочно находится в другом кластере;
    \item если значение близко к 0, то объект находится между кластерами.
\end{itemize}

Оптимальным считается такое количество кластеров, при котором достигаеся максимальное среднее значение коэффициентов силуэтов.


\section{Постановка задачи}

В рамках выполнения выпускной квалификационной работы требуется реализовать метод разбиения категориальных данных на основе агломеративного подхода иерархический кластеризации. При создании метода необходимо определить:
\begin{itemize}
    \item входные и выходные данные алгоритма кластеризации;
    \item критерий связи кластеров в иерархической части разрабатываемого метода;
    \item меру расстояний при вычислении матрицы несходста.
\end{itemize}

Иерархические методы возвращают бинарное дерево кластеров, узлы которого содержат номера объектов, входящих в данную группу. На основе этих узлов можно вычислить центры кластеров, используемые в центроидных методах разбиения.
Поэтому именно кластеризация центроидного типа может быть использованы для уточнения результатов иерархического разбиения в разрабатываемом гибридном методе.

Формальная постановка задачи в виде $IDEF0$-диаграммы представлена на рисунках \ref{img:idef0_a0}--\ref{img:idef0_a1}.
\imgs{idef0_a0}{h!}{1}{IDEF0–диаграмма уровня A0}
\imgs{idef0_a1}{h!}{0.56}{IDEF0–диаграмма уровня A1}
\clearpage

\section*{Вывод}

В данном разделе была описана предметная область, разобраны методы разбиения данных, проведено их сравнение по выделенным критериям. Были рассмотрены меры расстояний, используемые для определения схожести между объектами, проведено их сравнение по типу обрабатываемых данных. Для иерархической кластеризации были рассмотрены критерии связи кластеров. Также была описана формальная постановка задачи и были разобраны методы оценки качества разбиения, с помощью которых будет производиться сравнения разрабатываемого метода с аналогами.
